{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4961b1",
   "metadata": {},
   "source": [
    "# Scrapowanie Airbnb przy użyciu API Scraping Fish\n",
    "\n",
    "Ten notebook to przykład jak można scrapować stronę renderującą swoją treść za pomocą JavaScriptu, korzystając z [API Scraping Fish](https://scrapingfish.pl). Będziemy pobierać dane ofert z Airbnb i dokonamy podstawowej eksploracji danych. Notebook ten jest związany z [postem na blogu](https://scrapingfish.pl/blog/scraping-airbnb), który stanowi dokładniejszy opis tego kodu.\n",
    "\n",
    "Żeby móc uruchomić ten notebook i faktycznie pobrać dane, będziesz potrzebować klucza API Scraping Fish, który możesz [kupić tutaj](https://scrapingfish.pl/buy). Podstawowa paczka requestów kosztuje 8zł + VAT i w zupełności wystarczy, żeby uruchomić ten notebook wiele razy. Bez tego, JavaScript nie wyrenderuje zawartości i nie będzie można jej pobrać ze strony.\n",
    "\n",
    "Scraping Fish to api do scrapowania oparte o rotujące mobilne proxy 4G/LTE. To najlepszy typ proxy do scrapowania ponieważ mobilne adresy IP są ulotne i często zmieniają się pomiędzy prawdziwmy użytkownikami. Ten typ proxy jest w stanie zapewnić nieprzerwane scrapowanie nawet dla najbardziej wymagających stron bez bycia zablokowanym. Możesz przeczytać więcej o API Scraping Fish w [Dokumentacji](https://scrapingfish.pl/docs/intro).\n",
    "\n",
    "## Importy\n",
    "Potrzebne pakiety, które importujemy poniżej są wylistowane w pliku `requirements.txt`. Przed uruchomieniem notebooka zainstaluj je: `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c07f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote_plus\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(font_scale=1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65050b56",
   "metadata": {},
   "source": [
    "## Klucz API\n",
    "Klucz API Scraping Fish jest wymagany do uruchomienia tego przykładu dlatego, że dzięki temu będziemy mogli renderować JavaScript. Jedyne co jest wymagane do integracji to dodanie poniższego prefixu do każdego scrapowanego urla.\n",
    "\n",
    "Możesz [zdobyć klucz API](https://scrapingfish.pl/buy) kupując najtańszą paczkę za 8 zł + VAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f320cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"[Your Scraping Fish API key]\"\n",
    "url_prefix = f\"https://scraping.narf.ai/api/v1/?render_js=true&api_key={API_KEY}&url=\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc7d73e",
   "metadata": {},
   "source": [
    "## Parametry\n",
    "Poniżej definiujemy parametry dla naszego scrapowania. W tym przykładzie pobieramy oferty z Warszawy dostępne między 1 czerwca 2022 a 14 października 2022, pobierając z częstotliwością 45 dni. Możesz zmienić te parametry na jakiekolwiek chcesz, ale miej na uwadze, że zmiana częstotliwości (`frequency`) do małej wartości (np. `\"1D\"`) znacznie wydłuży czas scrapowania i niekoniecznie zwiększy ilość zebranych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93396c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Warszawa\"\n",
    "start_date = datetime.date(2022, 6, 1)\n",
    "end_date = datetime.date(2022, 10, 14)\n",
    "freq = '45D'\n",
    "num_guests = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4515eaf5",
   "metadata": {},
   "source": [
    "## Scrapowanie i Parsowanie\n",
    "Dla danego zapytania Airbnb listuje 300 ofert (15 stron, 20 ofert na każdej stronie). Będziemy scrapować wszystkie 15 stron dla każdego dnia, wyciągać informacje z każdej z ofert i dodawać je do kolekcji.\n",
    "\n",
    "### Parsowanie pojedynczej oferty\n",
    "Najpierw zdefiniujemy funkcję odpowiedzialną za parsowanie pojedycznej oferty. Będzie ona wywoływana dla każdej oferty na stronie żeby wydobyć interesujące nas informacje i zapisać je do słownika, który potem będzie dodany do kolekcji wszystkich ofert. Musimy też zdefiniować mapowanie kluczy, żeby ustandaryzować m.in. liczbę mnogą i pojedynczą do wspólnego klucza.\n",
    "\n",
    "BeatufiulSoup jest tu używany do wybierania pod-elementów z elementu głównego zawierającego informacje o ofercie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2edb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_map = {\n",
    "    'łazienka': 'bathrooms',\n",
    "    'łazienki': 'bathrooms',\n",
    "    'łazienek': 'bathrooms',\n",
    "    'łazienki': 'bathrooms',\n",
    "    'łazieniek': 'bathrooms', # yup, a typo on Airbnb's website\n",
    "    'współdzielona łazienka': 'bathrooms',\n",
    "    'współdzielone łazienki': 'bathrooms',\n",
    "    'współdzielonych łazienek': 'bathrooms',\n",
    "    'wspólnej łazienki': 'bathrooms',\n",
    "    'wspólnych łazienek': 'bathrooms',\n",
    "    'prywatna łazienka': 'bathrooms',\n",
    "    'prywatne łazienki': 'bathrooms',\n",
    "    'prywatnych łazienek': 'bathrooms',\n",
    "    'toaleta': 'bathrooms',\n",
    "    'toalet': 'bathrooms',\n",
    "    'toalety': 'bathrooms',\n",
    "    'gość': 'capacity',\n",
    "    'gości': 'capacity',\n",
    "    'sypialnia': 'bedrooms',\n",
    "    'sypialnie': 'bedrooms',\n",
    "    'sypialni': 'bedrooms',\n",
    "    'łóżko': 'beds',\n",
    "    'łóżka': 'beds',\n",
    "    'łóżek': 'beds',\n",
    "}\n",
    "\n",
    "def parse_offer(offer, checkin):\n",
    "    offer_id = offer.select(\"a\")[0]['target'].split(\"listing_\")[-1]\n",
    "    offer_price = float(''.join(offer.select(\"span._tyxjp1\")[0].text.split()[:-1]))\n",
    "    offer_type = \" \".join(offer.select(\"div.mj1p6c8\")[0].text.split(\"w:\")[0].split())\n",
    "    offer_features = [feature.text for feature in offer.select(\"span.mp2hv9t\")]\n",
    "\n",
    "    current_offer = {\n",
    "        \"id\": offer_id,\n",
    "        \"price\": offer_price,\n",
    "        \"checkin\": checkin,\n",
    "        \"type\": offer_type,\n",
    "    }\n",
    "    feature_sets = offer.select(\"div.i1wgresd\")\n",
    "    basic_features = [t.text.lower() for t in feature_sets[0].select(\"span.mp2hv9t\")]\n",
    "    other_features = [t.text.lower() for t in feature_sets[1].select(\"span.mp2hv9t\")] if len(feature_sets) > 1 else []\n",
    "\n",
    "    for feature in basic_features:\n",
    "        split = feature.split()\n",
    "        if len(split) > 1:\n",
    "            current_offer[key_map[' '.join(split[1:])]] = split[0]\n",
    "        else:\n",
    "            if split[0].lower() == 'studio':\n",
    "                current_offer['bedrooms'] = 0\n",
    "\n",
    "    current_offer['wi-fi'] = 'wi-fi' in other_features\n",
    "    current_offer['kitchen'] = 'kuchnia' in other_features\n",
    "    current_offer['washing machine'] = 'pralka' in other_features\n",
    "    current_offer['self check in'] = 'samodzielne zameldowanie' in other_features\n",
    "    return current_offer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126714f",
   "metadata": {},
   "source": [
    "### Przetwarzanie danej daty\n",
    "Dla każdego dnia w ustalonym przedziale dat przetwarzamy go poprzez pobranie wszystkich 15 stron rezultatów. Dla każdej strony wyciągamy wszystkie elementy zawierające informacje o ofertach, parsujemy je (używając zdefiniowanej powyżej funkcji `parse_offer`) i dodajemy wynikowy słownik do kolekcji `offers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date(checkin_date, num_guests=1):\n",
    "    checkin = checkin_date.strftime(\"%Y-%m-%d\")\n",
    "    checkout = (checkin_date + datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    offers = []\n",
    "    total_num_pages = 15\n",
    "    page = 1\n",
    "    end = False\n",
    "    while page <= total_num_pages:\n",
    "        items_offset = (page - 1) * 20\n",
    "        url = f\"https://www.airbnb.pl/s/{query}/homes?checkin={checkin}&checkout={checkout}&adults={num_guests}&items_offset={items_offset}\"\n",
    "        url = quote_plus(url)\n",
    "        response = requests.get(f\"{url_prefix}{url}\")\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        for offer in soup.select(\"div.cm4lcvy\"):\n",
    "            current_offer = parse_offer(offer, checkin=checkin)\n",
    "            offers.append(current_offer)\n",
    "        \n",
    "        page += 1\n",
    "    return offers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6377fc61",
   "metadata": {},
   "source": [
    "### Proces scrapowania\n",
    "Teraz po prostu iterujemy po żądanym zakresie dat i przetwarzamy każdy dzień zgodnie z logiką zdefiniowaną w funkcjach powyżej. Zajmie to kilka minut, zależnie od zdefiniowanego zakresu dat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38dcda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "offers = []\n",
    "for date in tqdm(pd.date_range(start_date, end_date, freq=freq)):\n",
    "    offers += process_date(checkin_date=date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c2a31",
   "metadata": {},
   "source": [
    "## Eksploracja danych\n",
    "\n",
    "Mając dane w pamięci, stwórzmy data frame Pandas i dokonajmy eksploracji. Przy okazji zapiszemy nasze dane do pliku csv i wczytamy jego zawartość, co ma dodatkowy efekt w postaci automatycznej konwersji danych numerycznych na odpowiednie typy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a260d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "offers_df = pd.DataFrame(offers)\n",
    "offers_df.to_csv('./data-pl.csv', sep=';', index=False)\n",
    "offers_df = pd.read_csv(\"./data-pl.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb2e61",
   "metadata": {},
   "source": [
    "Oczyścimy najpierw dane z duplikatów zliczymy je i zliczymy różne typy ofert. Dodatkowo, w ofertach w których jest \"współdzielona\" łazienka zamienimy ilość na 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "offers_df = offers_df.drop_duplicates(subset=['id'], keep='last')\n",
    "print(\"Number of data points:\", len(offers_df))\n",
    "print(offers_df['type'].value_counts())\n",
    "offers_df[\"bathrooms\"].replace({\"współdzielona\": 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfedbe4",
   "metadata": {},
   "source": [
    "Jak widać jest wiele różnych szczegółowych typów, ale nas bardziej interesują szersze rodzaje stosowane przez Airbnb, tj.: \"Całe miejsce\", \"Pokój prywatny\", \"Pokój w hotelu\" i \"Pokój współdzielony\". W tym celu dokonamy mapowania szczegółowych typów na ogołne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6fa4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "offers_df[\"type\"].replace({\n",
    "    \"Cały obiekt – apartament\": \"Całe miejsce\",\n",
    "    \"Cały apartament z obsługą\": \"Całe miejsce\",\n",
    "    \"Pokój w hostelu\": \"Pokój prywatny\",\n",
    "    \"Cały obiekt – apartament gościnny\": \"Całe miejsce\",\n",
    "    \"Cały obiekt – condo\": \"Całe miejsce\",\n",
    "    \"Pokój w apartamencie z obsługą\": \"Pokój prywatny\",\n",
    "    \"Cały obiekt – loft\": \"Całe miejsce\",\n",
    "    \"Łóżka w hostelu\": \"Pokój współdzielony\",\n",
    "    \"Pokój w hotelu butikowym\": \"Pokój hotelowy\",\n",
    "    \"Cały obiekt – dom\": \"Całe miejsce\",\n",
    "    \"Cały obiekt – domek parterowy\": \"Całe miejsce\",\n",
    "    \"Cały obiekt – domek gościnny\": \"Całe miejsce\",\n",
    "    \"Cały obiekt – chatka\": \"Całe miejsce\",\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f57d50",
   "metadata": {},
   "source": [
    "### Jakie rodzaje ofert są najczęstsze? Ile łóżek spodziewać się dla danego typu?\n",
    "\n",
    "Możemy teraz narysować histogram, którzy wskaże nam ile znaleźliśmy ofert tych podstawowych typów i przy okazji sprawdźmy ile zazwyczaj łóżek oferowane jest w danym typie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "ax = sns.histplot(data=offers_df, x=\"type\", shrink=0.9, alpha=0.9, multiple=\"stack\", hue=\"beds\")\n",
    "for label in ax.get_xticklabels():\n",
    "    label.set_rotation(35)\n",
    "ax.set(xlabel=\"Rodzaj miejsca\", ylabel=\"Liczba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab07a2",
   "metadata": {},
   "source": [
    "Całe miejsca wyraźne dominują i jednocześnie widać, że większość oferuje jedno lub dwa łóżka.\n",
    "\n",
    "### Cena a maksymalna liczba gości i bardzo drogie oferty\n",
    "\n",
    "Sprawdzimy teraz jak ceny mają się do maksymalnej liczby gości w oferowanych miejscach. Prawie wszystkie oferty w naszym przypadku mieszczą się w cenie poniżej 2.500 zł, więc wyświetlimy tylko taki przedział:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b881897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "ax = sns.stripplot(x=\"capacity\", y=\"price\", hue=\"type\", dodge=True, data=offers_df)\n",
    "ax.set(ylim=[0, 2500], xlabel=\"Maksymalna liczba gości\", ylabel=\"Cena\")\n",
    "ax.legend().set_title(\"Rodzaj miejsca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b1bfe3",
   "metadata": {},
   "source": [
    "### Najdroższe oferty\n",
    "Poniżej pobieramy oferty droższe niż 2.500 zł i generujemy linki do nich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44160181",
   "metadata": {},
   "outputs": [],
   "source": [
    "offers_df[offers_df.price > 2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035c84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for offer_id in offers_df[offers_df.price > 1500].id:\n",
    "    print(f\"https://airbnb.pl/rooms/{offer_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6205d3",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "Airbnb jest przykładem strony, która nie wyświetla prawie wcale wartościowej treści bez włączonego javascriptu. Funkcjonalność Scraping Fish [renderowania javascriptu](https://scrapingfish.pl/docs/api-options) sprawia, że poradzenie sobie z tym problemem to bułka z masłem.\n",
    "\n",
    "Ledwie dotknęliśmy zagadnienia eksploracji naszych danych. Żeby zachować prostotę tego posta ograniczyliśmy nasze analizy do stosunkowo małego zbioru danych, żeby zaprezentować co jest możliwe oraz jak łatwo można scrapować strony mocno oparte o javascript. Planujemy jednak pobrać o wiele więcej danych i przeprowadzić znacznie bardziej wnikliwe analizy, więc bądźcie czujni i sprawdzajcie naszego bloga!\n",
    "\n",
    "Jeśli też scrapujesz dane i budujesz przy użyciu tej technologii produkty sprawdź API Scraping Fish. Możesz [zacząć](https:/scrapingfish.pl/buy) już za 8 zł + VAT bez żadnych zobowiązań."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
