{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e67ce7",
   "metadata": {},
   "source": [
    "# Scraping Airbnb by Scraping Fish\n",
    "\n",
    "This notebook is an example of how you can scrape a website which uses JavaScript to render its content by using [Scraping Fish API](https://scrapingfish.com). We will extract Airbnb's offers data and perform some basic data visualization. It accompanies [the blog post](https://scrapingfish.com/blog/scraping-airbnb) which explains everything in more detailed manner.\n",
    "\n",
    "To be able to run this notebook and actually scrape the data, you will need Scraping Fish API key which you can [buy here](https://scrapingfish.com/buy). A starter pack costing just $2 will be more than enough to run the notebook many times. Without it, JavaScript will not be rendered and therefore no content will be available on Airbnb's website. \n",
    "\n",
    "Scraping Fish is an API for scraping powered by rotating 4G/LTE mobile proxy by default. It is the best available proxy type for scraping since mobile IPs are ephemeral and constantly reassigned between real users. This type of proxy is capable of scraping even the most demanding websites, without being blocked. You can read more on advanced topics in Scraping Fish API [Documentation](https://scrapingfish.com/docs/intro)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81b3497",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Required packages imported in the cell below are listed in requirements.txt file. Install them first by running `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b1ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote_plus\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(font_scale=1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f916fb0",
   "metadata": {},
   "source": [
    "## API key\n",
    "Scraping Fish API key is needed to run this example as it allows rendering JS during scraping easily. The API integration only requires adding the prefix to all scraped urls.\n",
    "\n",
    "You can [get API key](https://scrapingfish.com/buy) by buying the cheapest request pack for $2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f320cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"[Your API key]\"\n",
    "url_prefix = f\"https://scraping.narf.ai/api/v1/?render_js=true&api_key={API_KEY}&url=\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5314c716",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Here we define parameters of our query. In this example we scrape San Francisco Bay Area offers available between 2022/06/01 and 2022/10/14, sampling every 45th day. You can change these parameters to whatever you prefer but keep in mind that changing the frequency to lower values (e.g. `\"1D\"`) will result in much longer scraping and not necessarily much better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93396c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"San-Francisco-Bay-Area--CA--United-States\"\n",
    "start_date = datetime.date(2022, 6, 1)\n",
    "end_date = datetime.date(2022, 10, 14)\n",
    "freq = '45D'\n",
    "num_guests = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6388c583",
   "metadata": {},
   "source": [
    "## Scraping and Parsing\n",
    "For a given query Airbnb lists 300 offers (15 pages, 20 offers per page). We will scrape all those 15 pages per each processed day and extract info from each offer and append those to a collection.\n",
    "\n",
    "### Parsing individual offer\n",
    "First, let's define a function responsible for parsing an individual offer. It will be called for each offer in a page to extract the relevant info to a dictionary which will then be appended to the collection of all offers. We also need to define a mapping to standardize plular and singular forms to a common one.\n",
    "\n",
    "BeatufiulSoup is used here to select subelements from the top offer's element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3f81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_map = {\n",
    "    'bathrooms': 'bathrooms',\n",
    "    'bathroom': 'bathrooms',\n",
    "    'baths': 'bathrooms',\n",
    "    'bath': 'bathrooms',\n",
    "    'shared baths': 'bathrooms',                    \n",
    "    'shared bath': 'bathrooms',\n",
    "    'private baths': 'bathrooms',                    \n",
    "    'private bath': 'bathrooms',\n",
    "    'half-bath': 'bathrooms',\n",
    "    'bedrooms': 'bedrooms',\n",
    "    'bedroom': 'bedrooms',\n",
    "    'private bedrooms': 'bedrooms',\n",
    "    'private bedroom': 'bedrooms',                    \n",
    "    'beds': 'beds',\n",
    "    'bed': 'beds',\n",
    "    'guest': 'capacity',\n",
    "    'guests': 'capacity',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736f3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(offer_id, checkin, checkout):\n",
    "    url = quote_plus(f\"https://www.airbnb.com/rooms/{offer_id}?adults=1&check_in={checkin}&check_out={checkout}\")\n",
    "    response = requests.get(f\"{url_prefix}{url}\", timeout=120)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    additional_features = {}\n",
    "    \n",
    "    basic_features = [el.text for el in soup.select(\"li.len26si > span\")]\n",
    "    other_features = \" \".join([el.text.lower() for el in soup.select(\"div._1byskwn\")])\n",
    "    \n",
    "    for feature in basic_features:\n",
    "        split = feature.split()\n",
    "        if len(split) > 1:\n",
    "            key = ' '.join(split[1:])\n",
    "            if key == 'room types':\n",
    "                continue\n",
    "            additional_features[key_map[key]] = split[0]\n",
    "        else:\n",
    "            if split[0].lower() == 'studio':\n",
    "                additional_features['bedrooms'] = 0         \n",
    "    \n",
    "    additional_features['wi-fi'] = 'wifi' in other_features\n",
    "    additional_features['kitchen'] = 'kitchen' in other_features\n",
    "    additional_features['washer'] = 'washer' in other_features\n",
    "    additional_features['free parking'] = 'free parking' in other_features\n",
    "\n",
    "    return additional_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dbe34e",
   "metadata": {},
   "source": [
    "### Date processing\n",
    "For each date in the given date range we process it by querying all the 15 pages of results. For each page, we extract all elements containing offer information, parse it (using the `parse_offer` function defined above) and append the resulting dictionary to the `offers` collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date(checkin_date, num_guests=1):\n",
    "    checkin = checkin_date.strftime(\"%Y-%m-%d\")\n",
    "    checkout = (checkin_date + datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    offers = []\n",
    "    total_num_pages = 15\n",
    "    page = 1\n",
    "    while page <= total_num_pages:\n",
    "        items_offset = (page - 1) * 20\n",
    "        url = f\"https://www.airbnb.com/s/{query}/homes?checkin={checkin}&checkout={checkout}&adults={num_guests}&items_offset={items_offset}&display_currency=USD\"\n",
    "        url = quote_plus(url)\n",
    "        response = requests.get(f\"{url_prefix}{url}\", timeout=120)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        try:\n",
    "            prices = [el.text.split()[0][1:] for el in soup.select(\"._tt122m\")]\n",
    "            types = [el.text.split(\" in\")[0] for el in soup.select(\"div.t1jojoys\")]\n",
    "            ids = [el['target'].split(\"listing_\")[-1] for el in soup.select(\"a.lwm61be\")]\n",
    "            \n",
    "            for offer_id, offer_price, offer_type in zip(ids, prices, types):\n",
    "                offer = {\n",
    "                    \"id\": offer_id,\n",
    "                    \"price\": offer_price,\n",
    "                    \"checkin\": checkin,\n",
    "                    \"type\": offer_type,\n",
    "                }\n",
    "                \n",
    "                additional_features = extract_info(offer_id, checkin, checkout)\n",
    "                offer.update(additional_features)\n",
    "                offers.append(offer)\n",
    "            page += 1\n",
    "        except Exception as e:\n",
    "            with open(f\"{items_offset}.html\", \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            raise e\n",
    "    return offers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50363f",
   "metadata": {},
   "source": [
    "### Scraping process\n",
    "Now we just iterate over the desired date range and process each date according to the logic defined in functions above. It will take a couple of minutes depending on the range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38dcda7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "offers = []\n",
    "for date in tqdm(pd.date_range(start_date, end_date, freq=freq)):\n",
    "   offers += process_date(checkin_date=date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bbebd6",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Now that we have the data ready in memory, let's create a pandas dataframe and explore it. We will also write our data to csv and load it which has an additional benefit of auto-conversion of numeric data to proper types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96720003",
   "metadata": {},
   "outputs": [],
   "source": [
    "offers_df = pd.DataFrame(offers)\n",
    "offers_df.to_csv('./data.csv', sep=';', index=False)\n",
    "offers_df = pd.read_csv(\"./data.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb2e61",
   "metadata": {},
   "source": [
    "We will clear duplicated ads first, count the data and count values by type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "offers_df = offers_df.drop_duplicates(subset=['id'], keep='last')\n",
    "print(\"Number of data points:\", len(offers_df))\n",
    "print(offers_df['type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfedbe4",
   "metadata": {},
   "source": [
    "Here we see a fair count of detailed types but we are more insterested in broader type, i.e. we want only to count: \"Entire place\", \"Private room\", \"Hotel room\" and \"Shared room\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6fa4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "offers_df[\"type\"].replace({\n",
    "    \"Entire home\": \"Entire place\",\n",
    "    \"Entire guest suite\": \"Entire place\",\n",
    "    \"Entire guesthouse\": \"Entire place\",\n",
    "    \"Entire rental unit\": \"Entire place\",\n",
    "    \"Entire cottage\": \"Entire place\",\n",
    "    \"Entire condo\": \"Entire place\",\n",
    "    \"Entire cabin\": \"Entire place\",\n",
    "    \"Entire bungalow\": \"Entire place\",\n",
    "    \"Tiny home\": \"Entire place\", \n",
    "    \"Entire villa\": \"Entire place\",\n",
    "    \"Treehouse\": \"Entire place\",\n",
    "    \"Entire vacation home\": \"Entire place\",\n",
    "    \"Entire apartment\": \"Entire place\",\n",
    "    \"Boat\": \"Entire place\",\n",
    "    \"Houseboat\": \"Entire place\",\n",
    "    \"Farm stay\": \"Entire place\",\n",
    "    \"Tower\": \"Entire place\",\n",
    "    \"Entire loft\": \"Entire place\",\n",
    "    \"Entire townhouse\": \"Entire place\",\n",
    "    \"Bus\": \"Entire place\",\n",
    "    \"Camper/RV\": \"Entire place\",\n",
    "    \"Yurt\": \"Entire place\",\n",
    "    \"Tent\": \"Entire place\",\n",
    "    \"Entire serviced apartment\": \"Entire place\",\n",
    "    \"Room\": \"Private room\",\n",
    "    \"Hostel room\": \"Private room\",\n",
    "    \"Hostel beds\": \"Shared room\",\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc116da",
   "metadata": {},
   "source": [
    "### What types of offers are the most common? How many beds is expected per type?\n",
    "Now, let's see how many of those broader types offers are there and at the same type find out how many beds those type typically offer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "ax = sns.histplot(data=offers_df, x=\"type\", shrink=0.9, alpha=0.9, multiple=\"stack\", hue=\"beds\")\n",
    "for label in ax.get_xticklabels():\n",
    "    label.set_rotation(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab07a2",
   "metadata": {},
   "source": [
    "Whole places clearly dominate the space and most of the places have 1 or 2 beds. Let's now see how prices relate to capacity of the offers:\n",
    "\n",
    "### Price to max. number of guests and most expensives offers\n",
    "\n",
    "We will now check how prices relate to guest capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b881897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "ax = sns.stripplot(data=offers_df, x=\"capacity\", y=\"price\", dodge=True, hue=\"type\")\n",
    "ax.set(xlabel=\"Max. number of guests\", ylabel=\"Price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf62e2",
   "metadata": {},
   "source": [
    "### The most expensive offers\n",
    "Below we get the offers priced over $1,500 and generate links to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "offers_df[offers_df.price > 1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a638c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "for offer_id in offers_df[offers_df.price > 1500].id:\n",
    "    print(f\"https://airbnb.com/rooms/{offer_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aba92d",
   "metadata": {},
   "source": [
    "# Wrap up\n",
    "\n",
    "Airbnb is an example website which doesn't show much content without javascript enabled. Scraping Fish [javascript rendering](https://scrapingfish.com/docs/api-options) feature made dealing with this a breeze.\n",
    "\n",
    "We barely scratched the surface of what's possible when it comes to data exploration. For the sake of simplicity we restricted our analysis to relatively small dataset. However, we plan to get much more data and conduct a more thorough analysis so stay tuned and check our blog for more!\n",
    "\n",
    "If you are interested in scraping data and building products around it give Scraping Fish a try. You can [start](https://scrapingfish.com/buy) with only $2 and absolutely no commitment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
